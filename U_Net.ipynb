{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "U-Net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tao7p2-WfXBo"
      },
      "source": [
        "#%tensorflow_version 2.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu5nGtUpHH7H",
        "outputId": "314d6d67-ef02-49ca-92b7-69cce6526a8e"
      },
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import cv2\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "\n",
        "import numpy as np \n",
        "from numpy import expand_dims\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.utils import *\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tifffile as tiff\n",
        "\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "#import tensorflow.keras.backend as K\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 10392281426812461352\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 16183459840\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 14543393316521343394\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuaNMNY9HrY1"
      },
      "source": [
        "##Describe the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hUAfllGvtA1"
      },
      "source": [
        "class DataGen(Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "        self.n_batches = int(len(self.input_img_paths)/self.batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"float32\")\n",
        "\n",
        "\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "\n",
        "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE).reshape(self.img_size + (1,))\n",
        "            x[j] = img\n",
        " \n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE).reshape(self.img_size+(1,))\n",
        "            y[j] = img\n",
        "            \n",
        "        return x/256, y/256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaPokKhdHpfG"
      },
      "source": [
        "##Fetch data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hMq8WwAv47M"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset = \"ISBI2016\"\n",
        "\n",
        "path = \"/content/drive/MyDrive/\"+dataset+\"/trainx/\"\n",
        "path_aug = \"/content/drive/MyDrive/\"+dataset+\"/aug_trainx/\"\n",
        "path_y = \"/content/drive/MyDrive/\"+dataset+\"/trainy/\"\n",
        "path_augy = \"/content/drive/MyDrive/\"+dataset+\"/aug_trainy/\"\n",
        "\n",
        "augmented = False\n",
        "\n",
        "input_img_paths = [path + name for name in os.listdir(path)] ##+ ([path_aug + name for name in os.listdir(path_aug)] if augmented else [])\n",
        "target_img_paths = [path_y + name for name in os.listdir(path_y)] ##+ ([path_augy + name for name in os.listdir(path_augy)] if augmented else [])\n",
        "input_img_paths.sort()\n",
        "target_img_paths.sort()\n",
        "print(input_img_paths)\n",
        "print(target_img_paths)\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "\n",
        "#Get the img shape\n",
        "img_size = cv2.imread(input_img_paths[0], cv2.IMREAD_GRAYSCALE).shape\n",
        "print(img_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c88wve_yHbfy"
      },
      "source": [
        "##Describe architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7CEys1pHZWB"
      },
      "source": [
        "#input size, first layer of the architecture: 572x572\n",
        "def make_model(input_size, use_batch_normalization=False):\n",
        "  # first empty layer\n",
        "  input = layers.Input(input_size)\n",
        "  #The contracting path (left side of the architecture) follows the typical architecture of a convolutional network.\n",
        "  # It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU)\n",
        "  # and a 2x2 max pooling operation with stride 2 for downsampling\n",
        "  \n",
        "  cnn1 = layers.Conv2D(64,3, activation='relu', padding='same')(input)\n",
        "  if use_batch_normalization:\n",
        "    cnn1 = layers.BatchNormalization()(cnn1)\n",
        "  cnn1 = layers.Conv2D(64,3, activation='relu',padding='same')(cnn1)\n",
        "  pool1 = layers.MaxPooling2D(pool_size=(2, 2), strides=2)(cnn1)\n",
        "  cnn2 = layers.Conv2D(128,3, activation='relu',padding='same')(pool1)\n",
        "  if use_batch_normalization:\n",
        "    cnn2 = layers.BatchNormalization()(cnn2)\n",
        "  cnn2 = layers.Conv2D(128,3, activation='relu',padding='same')(cnn2)\n",
        "  pool2 = layers.MaxPooling2D(pool_size=(2, 2), strides=2)(cnn2)\n",
        "  cnn3 = layers.Conv2D(256,3, activation='relu',padding='same')(pool2)\n",
        "  if use_batch_normalization:\n",
        "    cnn3 = layers.BatchNormalization()(cnn3)\n",
        "  cnn3 = layers.Conv2D(256,3, activation='relu', padding='same')(cnn3)\n",
        "  pool3 = layers.MaxPooling2D(pool_size=(2, 2), strides=2)(cnn3)\n",
        "  cnn4 = layers.Conv2D(512,3, activation='relu', padding='same')(pool3)\n",
        "  if use_batch_normalization:\n",
        "    cnn4 = layers.BatchNormalization()(cnn4)\n",
        "  cnn4 = layers.Conv2D(512,3, activation='relu', padding='same')(cnn4)\n",
        "  pool4 = layers.MaxPooling2D(pool_size=(2, 2), strides=2)(cnn4)\n",
        "  cnn5 = layers.Conv2D(1024,3, activation='relu', padding='same')(pool4)\n",
        "  if use_batch_normalization:\n",
        "    cnn5 = layers.BatchNormalization()(cnn5)\n",
        "  cnn5 = layers.Conv2D(1024,3, activation='relu', padding='same')(cnn5)\n",
        "\n",
        "\n",
        "  #Drop-out layers at the end of the contracting path perform further implicit data augmentation.\n",
        "  drop_cnn5 = layers.Dropout(0.5)(cnn5)\n",
        "\n",
        "  #Every step in the expansive path consists of an upsampling of the feature map FOLLOWED (so conv*unsamp) by a 2x2 convolution (“up-convolution”) \n",
        "  #that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, \n",
        "  #and two 3x3 convolutions, each fol- lowed by a ReLU.\n",
        "\n",
        "  # P.S. In order to avoid to crop, because I was not sure on how to do it, we just need to put padding='same' \n",
        "  cnn_up1 = layers.Conv2D(512,2, activation='relu', padding='same')(layers.UpSampling2D(size = (2,2))(cnn5))\n",
        "  conc1 = layers.Concatenate(axis=3)([cnn4,cnn_up1])\n",
        "  cnn6 = layers.Conv2D(512, 3, activation = 'relu', padding='same')(conc1)\n",
        "  if use_batch_normalization:\n",
        "    cnn6 = layers.BatchNormalization()(cnn6)\n",
        "  cnn6 = layers.Conv2D(512, 3, activation = 'relu', padding='same')(cnn6)\n",
        "\n",
        "  cnn_up2 = layers.Conv2D(256,2, activation='relu', padding='same')(layers.UpSampling2D(size = (2,2))(cnn6))\n",
        "  conc2 = layers.Concatenate(axis=3)([cnn3,cnn_up2])\n",
        "  cnn7 = layers.Conv2D(256, 3, activation = 'relu', padding='same')(conc2)\n",
        "  if use_batch_normalization:\n",
        "    cnn7 = layers.BatchNormalization()(cnn7)\n",
        "  cnn7 = layers.Conv2D(256, 3, activation = 'relu', padding='same')(cnn7)\n",
        "\n",
        "  cnn_up3 = layers.Conv2D(128,2, activation='relu', padding='same')(layers.UpSampling2D(size = (2,2))(cnn7))\n",
        "  conc3 = layers.Concatenate(axis=3)([cnn2,cnn_up3])\n",
        "  cnn8 = layers.Conv2D(128, 3, activation = 'relu', padding='same')(conc3)\n",
        "  if use_batch_normalization:\n",
        "    cnn8 = layers.BatchNormalization()(cnn8)\n",
        "  cnn8 = layers.Conv2D(128, 3, activation = 'relu', padding='same')(cnn8)\n",
        "\n",
        "  cnn_up4 = layers.Conv2D(64,2, activation='relu', padding='same')(layers.UpSampling2D(size = (2,2))(cnn8))\n",
        "  conc4 = layers.Concatenate(axis=3)([cnn1,cnn_up4])\n",
        "  cnn9 = layers.Conv2D(64, 3, activation = 'relu', padding='same')(conc4)\n",
        "  if use_batch_normalization:\n",
        "    cnn9 = layers.BatchNormalization()(cnn9)\n",
        "  cnn9 = layers.Conv2D(64, 3, activation = 'relu', padding='same')(cnn9)\n",
        "  conv9 = layers.Conv2D(2, 3, activation = 'relu', padding = 'same')(cnn9)\n",
        "  cnn10 = layers.Conv2D(1, 1, activation = 'sigmoid')(cnn9)\n",
        "\n",
        "  model = keras.Model(inputs = input, outputs = cnn10)\n",
        "\n",
        "# I am using the base class for keras optimazers, see for more https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "  model.compile(optimizer = keras.optimizers.Adam(learning_rate=1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veY9tn0IHNP5"
      },
      "source": [
        "##Split data and create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv3hkgd57Gg9",
        "outputId": "7c0608de-2e48-4edc-96ce-5d9fa6ccdcbe"
      },
      "source": [
        "#Split data\n",
        "val_samples = int(len(target_img_paths)*0.7)\n",
        "test_samples = int(len(target_img_paths)*0.85)\n",
        "\n",
        "train_input_img_paths = input_img_paths[:val_samples]\n",
        "train_target_img_paths = target_img_paths[:val_samples]\n",
        "val_input_img_paths = input_img_paths[val_samples:test_samples]\n",
        "val_target_img_paths = target_img_paths[val_samples:test_samples]\n",
        "test_input_img_paths = input_img_paths[test_samples:]\n",
        "test_target_img_paths = target_img_paths[test_samples:]\n",
        "\n",
        "print(len(train_input_img_paths),\"train,\",len(val_input_img_paths),\"validation,\",len(test_input_img_paths),\"test\")\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_gen = DataGen(batch_size, img_size, train_input_img_paths, train_target_img_paths)\n",
        "val_gen = DataGen(1, img_size, val_input_img_paths, val_target_img_paths)\n",
        "test_gen = DataGen(1, img_size, test_input_img_paths, test_target_img_paths)\n",
        "\n",
        "# Create a model\n",
        "model = make_model(img_size+(1,), use_batch_normalization=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "630 train, 135 validation, 135 test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eSDBxosHxTc"
      },
      "source": [
        "##Functions for measuring performances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UMKssGH1cj3"
      },
      "source": [
        "measuring_loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "def output_image(data, model, title='', save_path=''):\n",
        "  (X, Y) = data\n",
        "  X = X[0:1] #Take just one img from the batch\n",
        "  Y = Y[0:1]\n",
        "  model_output = model.predict(X)\n",
        "  f, axarr = plt.subplots(1,3, figsize=(8, 6))\n",
        "  axarr[0].imshow(X.reshape(img_size))\n",
        "  axarr[0].title.set_text('Input '+title)\n",
        "  axarr[1].imshow(model_output.reshape(img_size))\n",
        "  axarr[1].title.set_text('Output (loss='+str(round(measuring_loss(Y, model_output).numpy(), 5))+')')\n",
        "  axarr[2].imshow(Y.reshape(img_size))\n",
        "  axarr[2].title.set_text('Target output')\n",
        "\n",
        "  if save_path != '':\n",
        "    f.savefig(save_path, dpi=100, bbox_inches='tight')\n",
        "  plt.close(f)\n",
        "\n",
        "def measure_avg_loss(generator, model, max_images_mesures=20):\n",
        "  avg_loss = 0\n",
        "  for i in range(generator.n_batches):\n",
        "    if i * generator.batch_size > max_images_mesures: #Mesurer sur maxi 20 images\n",
        "      break\n",
        "    (X, Y) = generator[i]\n",
        "    model_output = model.predict(X)\n",
        "    avg_loss += measuring_loss(Y, model_output).numpy() / generator.n_batches\n",
        "  return avg_loss\n",
        "\n",
        "def plot_training(epochs, losses, val_losses, save_path):\n",
        "  plt.figure(figsize=(6, 4))\n",
        "  # summarize history for loss\n",
        "  plt.plot(epochs,losses)\n",
        "  plt.plot(epochs,val_losses)\n",
        "  plt.title('Training curves')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  if save_path != '':\n",
        "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
        "  plt.close()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVfxggCIHvlG"
      },
      "source": [
        "##Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54g-aNFjHTUA"
      },
      "source": [
        "if False:\n",
        "  save_model_path = \"/content/drive/MyDrive/saved_models/augmented/model-{epoch:03d}\"\n",
        "\n",
        "  callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor='loss', patience=3),\n",
        "    keras.callbacks.ModelCheckpoint(filepath= save_model_path,save_weights_only=True, verbose=0, save_freqs='epochs')\n",
        "  ]\n",
        "  # Train the model, doing validation at the end of each epoch.\n",
        "  epochs = 3\n",
        "\n",
        "  history = model.fit(train_gen, epochs=epochs, verbose=1, shuffle=True, validation_data=val_gen, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAgcliAXL11W"
      },
      "source": [
        "##Load already trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEJbzp6aL40a"
      },
      "source": [
        "if False:\n",
        "  save_model_path = \"/content/drive/MyDrive/saved_models/augmented/model-01\"\n",
        "  model = make_model(img_size+(1,), use_batch_normalization=False)\n",
        "  model.load_weights(save_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDP2e8caH0iR"
      },
      "source": [
        "##Measure performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QWN41EN5SLA",
        "outputId": "29811ce1-cf17-4fca-e91b-461db90fdd71"
      },
      "source": [
        "model = make_model(img_size+(1,), use_batch_normalization=True)\n",
        "save_path = \"/content/drive/MyDrive/saved_models/with_batch_norm_2016/\"\n",
        "losses = []\n",
        "val_losses = []\n",
        "#\"weights/model-{epoch:03d}\"\n",
        "history = None\n",
        "\n",
        "epoch_jump = 1\n",
        "epochs = [epoch_jump*i for i in range(20)]\n",
        "\n",
        "for epoch in epochs:\n",
        "  if history != None:\n",
        "    losses += history.history['loss']\n",
        "    val_losses += history.history['val_loss']\n",
        "    plot_training(epochs[:epoch], losses,val_losses, save_path=save_path+'epoch_'+str(epoch)+'_training')\n",
        "\n",
        "  output_image(train_gen[0], model, title='(train set)', save_path=save_path+'epoch_'+str(epoch)+'_train')\n",
        "  output_image(val_gen[0], model, title='(validation set)', save_path=save_path+'epoch_'+str(epoch)+'_val')\n",
        "  output_image(test_gen[0], model, title='(test set)', save_path=save_path+'epoch_'+str(epoch)+'_test')\n",
        "\n",
        "  train_loss = round(measure_avg_loss(train_gen, model), 5)\n",
        "  valid_loss = round(measure_avg_loss(val_gen, model), 5)\n",
        "  test_loss = round(measure_avg_loss(test_gen, model), 5)\n",
        "  \n",
        "  file1 = open(save_path+'epoch_'+str(epoch)+'_losses.txt',\"w\")#append mode\n",
        "  file1.write(str(train_loss)+' '+str(valid_loss)+' '+str(test_loss))\n",
        "  file1.close()\n",
        "\n",
        "  checkpoint = keras.callbacks.ModelCheckpoint(filepath=save_path+'/weights/epoch_'+str(epoch),save_weights_only=True, verbose=0, save_freqs='epochs')\n",
        "  history = model.fit(train_gen, epochs=epoch_jump, verbose=1, shuffle=True, validation_data=val_gen, callbacks=[checkpoint])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 141s 861ms/step - loss: 0.3117 - accuracy: 0.6739 - val_loss: 0.6230 - val_accuracy: 0.7425\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.2312 - accuracy: 0.6875 - val_loss: 1.1033 - val_accuracy: 0.7425\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.2006 - accuracy: 0.6893 - val_loss: 0.2197 - val_accuracy: 0.7264\n",
            "157/157 [==============================] - 117s 746ms/step - loss: 0.1780 - accuracy: 0.6935 - val_loss: 0.3574 - val_accuracy: 0.5970\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.1797 - accuracy: 0.6927 - val_loss: 0.1360 - val_accuracy: 0.7310\n",
            "157/157 [==============================] - 117s 745ms/step - loss: 0.1516 - accuracy: 0.6944 - val_loss: 0.2554 - val_accuracy: 0.6602\n",
            "157/157 [==============================] - 117s 745ms/step - loss: 0.1538 - accuracy: 0.6941 - val_loss: 0.2667 - val_accuracy: 0.6603\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.1331 - accuracy: 0.6964 - val_loss: 0.1360 - val_accuracy: 0.7349\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.1197 - accuracy: 0.6979 - val_loss: 0.2244 - val_accuracy: 0.6883\n",
            "157/157 [==============================] - 117s 745ms/step - loss: 0.1239 - accuracy: 0.6976 - val_loss: 0.1295 - val_accuracy: 0.7315\n",
            "157/157 [==============================] - 117s 745ms/step - loss: 0.1126 - accuracy: 0.6989 - val_loss: 1.3370 - val_accuracy: 0.3327\n",
            "157/157 [==============================] - 117s 745ms/step - loss: 0.1157 - accuracy: 0.6978 - val_loss: 0.1507 - val_accuracy: 0.7082\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.1020 - accuracy: 0.6996 - val_loss: 0.6066 - val_accuracy: 0.4946\n",
            "157/157 [==============================] - 117s 746ms/step - loss: 0.1026 - accuracy: 0.6996 - val_loss: 0.1281 - val_accuracy: 0.7298\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.0832 - accuracy: 0.7022 - val_loss: 0.1111 - val_accuracy: 0.7272\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.0789 - accuracy: 0.7027 - val_loss: 0.1567 - val_accuracy: 0.7016\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.0751 - accuracy: 0.7032 - val_loss: 0.1524 - val_accuracy: 0.7314\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.0766 - accuracy: 0.7028 - val_loss: 0.1290 - val_accuracy: 0.7250\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.0753 - accuracy: 0.7031 - val_loss: 0.1688 - val_accuracy: 0.7111\n",
            "157/157 [==============================] - 117s 745ms/step - loss: 0.0705 - accuracy: 0.7040 - val_loss: 0.2534 - val_accuracy: 0.6920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pgnIMkJ4cC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}